{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e11d411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING AGENTIC PIPELINE ---\n",
      "\n",
      "[Header Agent] Scanning 'case_A1_sales_light_dirty_input.xlsx' for the true table coordinates...\n",
      "     [Header Agent Decision] Table starts at Row 3, Column 0.\n",
      "     [Reasoning] The data sample reveals that the first three rows contain meta-information and are not part of the data table, as they include titles, notes, and blank rows. \n",
      "\n",
      "The row with index 3 contains the actual column headers for the data table, identified by standard labels like 'Txn ID', 'Date', etc. These headers clearly mark the start of structured data.\n",
      "\n",
      "The actual data in the sample starts from column 0, with 'Txn ID' being the first column in the dataset. Thus, the 2D coordinate indicating the start of the data table is (3, 0).\n",
      "\n",
      "[Orchestrator] Loading file starting at row 3...\n",
      "\n",
      "[NA Agent] Scanning dataframe for custom missing values, empty rows, and empty columns...\n",
      "     [Header Agent Decision] Table starts at Row 3, Column 0.\n",
      "     [Reasoning] The data sample reveals that the first three rows contain meta-information and are not part of the data table, as they include titles, notes, and blank rows. \n",
      "\n",
      "The row with index 3 contains the actual column headers for the data table, identified by standard labels like 'Txn ID', 'Date', etc. These headers clearly mark the start of structured data.\n",
      "\n",
      "The actual data in the sample starts from column 0, with 'Txn ID' being the first column in the dataset. Thus, the 2D coordinate indicating the start of the data table is (3, 0).\n",
      "\n",
      "[Orchestrator] Loading file starting at row 3...\n",
      "\n",
      "[NA Agent] Scanning dataframe for custom missing values, empty rows, and empty columns...\n",
      "     [NA Agent Decision] Wipe strings: ['-'] | Drop rows: True | Drop cols: True\n",
      "       [Tool Executing] Cleaning custom NAs, empty rows, and empty columns...\n",
      "       [Tool Success] Wiped strings: ['-']\n",
      "       [Tool Success] Dropped 2 completely empty rows.\n",
      "[Reader Agent] Reading cropped dataframe to classify columns...\n",
      "     [NA Agent Decision] Wipe strings: ['-'] | Drop rows: True | Drop cols: True\n",
      "       [Tool Executing] Cleaning custom NAs, empty rows, and empty columns...\n",
      "       [Tool Success] Wiped strings: ['-']\n",
      "       [Tool Success] Dropped 2 completely empty rows.\n",
      "[Reader Agent] Reading cropped dataframe to classify columns...\n",
      "     [Reader Agent Classification] ['string', 'time', 'name', 'string', 'string', 'money', 'name', 'string']\n",
      "\n",
      "[Orchestrator] Data loaded. Delegating tasks...\n",
      "  -> [Orchestrator] Bypassing 'Txn ID' (Type: string requires no formatting)\n",
      "  -> [Time Agent] Taking control of column: 'Date'\n",
      "     [Reader Agent Classification] ['string', 'time', 'name', 'string', 'string', 'money', 'name', 'string']\n",
      "\n",
      "[Orchestrator] Data loaded. Delegating tasks...\n",
      "  -> [Orchestrator] Bypassing 'Txn ID' (Type: string requires no formatting)\n",
      "  -> [Time Agent] Taking control of column: 'Date'\n",
      "     [Time Agent Decision] The given data samples represent specific calendar dates. Each entry provides detailed information on the day, month, and year. The granularity of the data does not include hours or minutes, nor is it down to the month level without a day. Therefore, the most appropriate standardized format should include the day, month, and year, fitting the format of a complete date which matches \"%d/%m/%Y\".\n",
      "       [Tool Executing] Formatting 'Date' to '%d/%m/%Y'...\n",
      "       [Tool Success] Column updated.\n",
      "  -> [Name Agent] Taking control of column: 'Customer Name '\n",
      "     [Time Agent Decision] The given data samples represent specific calendar dates. Each entry provides detailed information on the day, month, and year. The granularity of the data does not include hours or minutes, nor is it down to the month level without a day. Therefore, the most appropriate standardized format should include the day, month, and year, fitting the format of a complete date which matches \"%d/%m/%Y\".\n",
      "       [Tool Executing] Formatting 'Date' to '%d/%m/%Y'...\n",
      "       [Tool Success] Column updated.\n",
      "  -> [Name Agent] Taking control of column: 'Customer Name '\n",
      "     [Name Agent Decision] Type: Locations/Other | Dominant Format: N/A\n",
      "       [Tool Executing] Cleaning names. Type: Locations/Other, Format: N/A...\n",
      "       [Tool Success] Column 'Customer Name ' standardized.\n",
      "  -> [Orchestrator] Bypassing 'Product' (Type: string requires no formatting)\n",
      "  -> [Orchestrator] Bypassing 'Region' (Type: string requires no formatting)\n",
      "  -> [Money Agent] Taking control of column: 'Amount (USD)'\n",
      "     [Name Agent Decision] Type: Locations/Other | Dominant Format: N/A\n",
      "       [Tool Executing] Cleaning names. Type: Locations/Other, Format: N/A...\n",
      "       [Tool Success] Column 'Customer Name ' standardized.\n",
      "  -> [Orchestrator] Bypassing 'Product' (Type: string requires no formatting)\n",
      "  -> [Orchestrator] Bypassing 'Region' (Type: string requires no formatting)\n",
      "  -> [Money Agent] Taking control of column: 'Amount (USD)'\n",
      "     [Money Agent Decision] Mixed: False | Currency: USD | Scale: None\n",
      "       [Tool Executing] Scale: None, Mixed Currency: False...\n",
      "       [Tool Success] Floats extracted. Renamed to 'Amount (USD) (USD)'.\n",
      "  -> [Name Agent] Taking control of column: 'Sales Rep'\n",
      "     [Money Agent Decision] Mixed: False | Currency: USD | Scale: None\n",
      "       [Tool Executing] Scale: None, Mixed Currency: False...\n",
      "       [Tool Success] Floats extracted. Renamed to 'Amount (USD) (USD)'.\n",
      "  -> [Name Agent] Taking control of column: 'Sales Rep'\n",
      "     [Name Agent Decision] Type: Human Names | Dominant Format: First Last\n",
      "       [Tool Executing] Cleaning names. Type: Human Names, Format: First Last...\n",
      "       [Tool Success] Column 'Sales Rep' standardized.\n",
      "  -> [Orchestrator] Bypassing 'Notes' (Type: string requires no formatting)\n",
      "\n",
      "  -> [Description Agent] Analyzing the final dataset to generate feature documentation...\n",
      "     [Name Agent Decision] Type: Human Names | Dominant Format: First Last\n",
      "       [Tool Executing] Cleaning names. Type: Human Names, Format: First Last...\n",
      "       [Tool Success] Column 'Sales Rep' standardized.\n",
      "  -> [Orchestrator] Bypassing 'Notes' (Type: string requires no formatting)\n",
      "\n",
      "  -> [Description Agent] Analyzing the final dataset to generate feature documentation...\n",
      "     [Description Agent] Summary: This dataset contains transactional sales information detailing each transaction's unique identifier, date, customer details, product specifics, sales region, financial figures, responsible sales representative, and any special instructions or notes.\n",
      "     [Description Agent] Successfully generated dictionary for 8 features.\n",
      "\n",
      "[Orchestrator] All tasks complete. Saved multi-sheet file to: cleaned_case_A1_sales_light_dirty_input.xlsx\n",
      "     [Description Agent] Summary: This dataset contains transactional sales information detailing each transaction's unique identifier, date, customer details, product specifics, sales region, financial figures, responsible sales representative, and any special instructions or notes.\n",
      "     [Description Agent] Successfully generated dictionary for 8 features.\n",
      "\n",
      "[Orchestrator] All tasks complete. Saved multi-sheet file to: cleaned_case_A1_sales_light_dirty_input.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class ColumnTypes(BaseModel):\n",
    "    types: list[str]\n",
    "\n",
    "def reader_agent(df: pd.DataFrame) -> list[str]:\n",
    "    # 1. We changed the input parameter to 'df: pd.DataFrame'\n",
    "    print(\"[Reader Agent] Reading cropped dataframe to classify columns...\")\n",
    "    \n",
    "    # 2. REMOVED the pd.read_excel() line entirely because the data is already loaded!\n",
    "    \n",
    "    # 3. Grab the sample directly from the passed dataframe\n",
    "    sample_data = df.head(5).to_dict(orient=\"list\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following data sample from an Excel file.\n",
    "    For each column, determine its data type based on the values.\n",
    "    You must return a list where each element corresponds to a column from left to right.\n",
    "    \n",
    "    You are ONLY allowed to use these exact categories: \"time\", \"money\", \"int\", \"string\", \"float\", \"name\", \"unknown\".\n",
    "    \n",
    "    CRITICAL DEFINITIONS:\n",
    "    - \"time\": Includes standard formats (2023-01-01, 14:30), timestamps, AND natural language dates (e.g., \"first of january 2016\", \"Q1 2024\", \"yesterday\"). If the core meaning represents a date or time, it is \"time\", NEVER \"string\".\n",
    "    - \"money\": Includes currency symbols ($100, €50), accounting formats, or financial abbreviations (100 USD) and natural language money expressions (\"100 dollars\", \"fifty euros\"). If the core meaning represents a monetary value, it is \"money\", NEVER \"string\".\n",
    "    - \"int\": Whole numbers without decimals.\n",
    "    - \"float\": Numbers containing decimals.\n",
    "    - \"name\": Proper nouns. This includes human names (John Smith, Smith, John), cities, states (Alabama), or company names.\n",
    "    - \"string\": General text, sentences, descriptions, or specific codes (e.g., ID-4552) that have no mathematical or temporal value.\n",
    "    - \"unknown\": Use this ONLY if the column is complete gibberish or you cannot confidently assign it to any other category.\n",
    "\n",
    "    Data sample (Columns and their first 5 values):\n",
    "    {sample_data}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a data classification agent.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=ColumnTypes\n",
    "    )\n",
    "    \n",
    "    types = response.choices[0].message.parsed.types\n",
    "    print(f\"     [Reader Agent Classification] {types}\")\n",
    "    return types\n",
    "\n",
    "class TimeFormatDecision(BaseModel):\n",
    "    reasoning: str\n",
    "    target_format: Literal[\n",
    "        \"%H:%M\", \"%H:%M:%S\", \"%S\", \n",
    "        \"%d/%m/%Y\", \"%d/%m/%Y %H:%M\", \"%d/%m/%Y %H:%M:%S\", \n",
    "        \"%m/%Y\", \"%Y\"\n",
    "    ]\n",
    "\n",
    "def execute_time_formatting(df: pd.DataFrame, col_name: str, target_format: str) -> pd.DataFrame:\n",
    "    \"\"\"The Tool used to physically alter the dataframe.\"\"\"\n",
    "    print(f\"       [Tool Executing] Formatting '{col_name}' to '{target_format}'...\")\n",
    "    \n",
    "    def parse_natural_language(date_str):\n",
    "        if pd.isna(date_str):\n",
    "            return pd.NaT\n",
    "            \n",
    "        clean_str = str(date_str).lower()\n",
    "        replacements = {\n",
    "            \"first\": \"1st\", \"second\": \"2nd\", \"third\": \"3rd\", \n",
    "            \"fourth\": \"4th\", \"fifth\": \"5th\", \"sixth\": \"6th\", \n",
    "            \"seventh\": \"7th\", \"eighth\": \"8th\", \"ninth\": \"9th\", \n",
    "            \"tenth\": \"10th\", \"eleventh\": \"11th\", \"twelfth\": \"12th\", \n",
    "            \"thirteenth\": \"13th\", \"fourteenth\": \"14th\", \"fifteenth\": \"15th\", \n",
    "            \"sixteenth\": \"16th\", \"seventeenth\": \"17th\", \"eighteenth\": \"18th\", \n",
    "            \"nineteenth\": \"19th\", \"twentieth\": \"20th\",\n",
    "            \"twenty-first\": \"21st\", \"twenty first\": \"21st\",\n",
    "            \"twenty-second\": \"22nd\", \"twenty second\": \"22nd\",\n",
    "            \"twenty-third\": \"23rd\", \"twenty third\": \"23rd\",\n",
    "            \"twenty-fourth\": \"24th\", \"twenty fourth\": \"24th\",\n",
    "            \"twenty-fifth\": \"25th\", \"twenty fifth\": \"25th\",\n",
    "            \"twenty-sixth\": \"26th\", \"twenty sixth\": \"26th\",\n",
    "            \"twenty-seventh\": \"27th\", \"twenty seventh\": \"27th\",\n",
    "            \"twenty-eighth\": \"28th\", \"twenty eighth\": \"28th\",\n",
    "            \"twenty-ninth\": \"29th\", \"twenty ninth\": \"29th\",\n",
    "            \"thirtieth\": \"30th\", \n",
    "            \"thirty-first\": \"31st\", \"thirty first\": \"31st\",\n",
    "            \"last\": \"last\"\n",
    "        }\n",
    "        for word, num in replacements.items():\n",
    "            clean_str = clean_str.replace(word, num)\n",
    "            \n",
    "        parsed = dateparser.parse(clean_str)\n",
    "        return parsed if parsed else pd.NaT\n",
    "\n",
    "    try:\n",
    "        df[col_name] = df[col_name].apply(parse_natural_language)\n",
    "        df[col_name] = df[col_name].dt.strftime(target_format)\n",
    "        print(f\"       [Tool Success] Column updated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"       [Tool Error] Failed: {e}\")\n",
    "    return df\n",
    "\n",
    "def time_agent_workflow(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    print(f\"  -> [Time Agent] Taking control of column: '{col_name}'\")\n",
    "    sample_data = df[col_name].dropna().head(5).tolist()\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Look at this sample of time/date data from the column '{col_name}'.\n",
    "    Data sample: {sample_data}\n",
    "    \n",
    "    Determine the appropriate standardized format for this data based on its granularity.\n",
    "    - Hours and minutes: \"%H:%M\"\n",
    "    - Hours, minutes, and seconds: \"%H:%M:%S\"\n",
    "    - Just seconds: \"%S\"\n",
    "    - Specific dates: \"%d/%m/%Y\"\n",
    "    - Date and time: \"%d/%m/%Y %H:%M\"\n",
    "    - Date and exact time: \"%d/%m/%Y %H:%M:%S\"\n",
    "    - Month and year: \"%m/%Y\"\n",
    "    - Year only: \"%Y\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert data formatting agent.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=TimeFormatDecision\n",
    "    )\n",
    "    \n",
    "    decision = response.choices[0].message.parsed\n",
    "    print(f\"     [Time Agent Decision] {decision.reasoning}\")\n",
    "    \n",
    "    df = execute_time_formatting(df, col_name, decision.target_format)\n",
    "    return df\n",
    "\n",
    "\n",
    "def orchestrator_router(df: pd.DataFrame, type_vector: list[str], original_filename: str):\n",
    "    print(f\"\\n[Orchestrator] Data loaded. Delegating tasks...\")\n",
    "    # REMOVED: df = pd.read_excel(file_path) because df is already loaded and cropped!\n",
    "    \n",
    "    for col_name, col_type in zip(df.columns, type_vector):\n",
    "        if col_type == \"time\":\n",
    "            df = time_agent_workflow(df, col_name)\n",
    "        elif col_type == \"money\":\n",
    "            df = money_agent_workflow(df, col_name) \n",
    "        elif col_type == \"int\":\n",
    "            df = int_agent_workflow(df, col_name)\n",
    "        elif col_type == \"float\":\n",
    "            # Added float logic here just in case you need it!\n",
    "            df = float_agent_workflow(df, col_name)\n",
    "        elif col_type == \"name\":\n",
    "            df = name_agent_workflow(df, col_name)\n",
    "        elif col_type in [\"string\", \"unknown\"]:\n",
    "            print(f\"  -> [Orchestrator] Bypassing '{col_name}' (Type: {col_type} requires no formatting)\")\n",
    "\n",
    "    desc_df = dataset_description_agent(df)\n",
    "    \n",
    "    # Save the file using pd.ExcelWriter to support multiple tabs\n",
    "    output_path = \"cleaned_\" + original_filename\n",
    "    \n",
    "    # We use engine='openpyxl' to ensure it writes modern .xlsx files properly\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        # Save the actual data to the first sheet\n",
    "        df.to_excel(writer, sheet_name=\"Cleaned_Data\", index=False)\n",
    "        \n",
    "        # Save the LLM's descriptions to the second sheet\n",
    "        desc_df.to_excel(writer, sheet_name=\"dataset_description\", index=False)\n",
    "        \n",
    "    print(f\"\\n[Orchestrator] All tasks complete. Saved multi-sheet file to: {output_path}\")\n",
    "\n",
    "class MoneyFormatDecision(BaseModel):\n",
    "    reasoning: str\n",
    "    is_mixed_currency: bool  \n",
    "    detected_currency: str   \n",
    "    scale_decision: Literal[\"None\", \"Thousands\", \"Millions\", \"Billions\"]\n",
    "    decimal_separator: Literal[\".\", \",\"]\n",
    "\n",
    "\n",
    "def execute_money_formatting(df: pd.DataFrame, col_name: str, decision: MoneyFormatDecision) -> pd.DataFrame:\n",
    "    print(f\"       [Tool Executing] Scale: {decision.scale_decision}, Mixed Currency: {decision.is_mixed_currency}...\")\n",
    "    \n",
    "    def parse_money_string(val):\n",
    "        if pd.isna(val):\n",
    "            return pd.NA, \"\"\n",
    "            \n",
    "        val_str = str(val).lower().strip()\n",
    "        original_str = str(val).strip() \n",
    "        \n",
    "        # 1. Extract the currency symbol, code, or full word\n",
    "        symbol_match = re.search(r'([\\$€£¥]|(?:usd|eur|gbp|jpy|dollars?|euros?|pounds?|yen))', original_str, re.IGNORECASE)\n",
    "        raw_symbol = symbol_match.group(1).lower() if symbol_match else \"\"\n",
    "        \n",
    "        # 2. Normalize the currency\n",
    "        currency_map = {\n",
    "            \"dollar\": \"USD\", \"dollars\": \"USD\", \"$\": \"USD\", \"usd\": \"USD\",\n",
    "            \"euro\": \"EUR\", \"euros\": \"EUR\", \"eur\": \"EUR\", \"€\": \"EUR\",\n",
    "            \"pound\": \"GBP\", \"pounds\": \"GBP\", \"gbp\": \"GBP\", \"£\": \"GBP\",\n",
    "            \"yen\": \"JPY\", \"jpy\": \"JPY\", \"¥\": \"JPY\"\n",
    "        }\n",
    "        symbol = currency_map.get(raw_symbol, raw_symbol.upper())\n",
    "        \n",
    "        # 3. Handle International Decimals\n",
    "        if decision.decimal_separator == \",\":\n",
    "            val_str = val_str.replace('.', '').replace(',', '.')\n",
    "        else:\n",
    "            val_str = val_str.replace(',', '')\n",
    "            \n",
    "        # DEFENSIVE SHIELD: Remove extra dots\n",
    "        if val_str.count('.') > 1:\n",
    "            parts = val_str.rsplit('.', 1)\n",
    "            val_str = parts[0].replace('.', '') + '.' + parts[1]\n",
    "            \n",
    "        # 4. Extract the core number\n",
    "        match = re.search(r'[\\d\\.]+', val_str)\n",
    "        if not match:\n",
    "            return pd.NA, symbol\n",
    "        try:\n",
    "            num = float(match.group())\n",
    "        except ValueError:\n",
    "            return pd.NA, symbol\n",
    "            \n",
    "        # 5. Apply word multipliers\n",
    "        isolated_words = re.sub(r'[\\d\\.\\,€\\$£¥]', ' ', val_str).split()\n",
    "        \n",
    "        if any(w in isolated_words for w in ['billion', 'billions', 'bill', 'bil', 'b']):\n",
    "            num *= 1_000_000_000\n",
    "        elif any(w in isolated_words for w in ['million', 'millions', 'mill', 'mil', 'm']):\n",
    "            num *= 1_000_000\n",
    "        elif any(w in isolated_words for w in ['thousand', 'thousands', 'k']):\n",
    "            num *= 1_000\n",
    "        elif any(w in isolated_words for w in ['cent', 'cents']):\n",
    "            num /= 100\n",
    "            \n",
    "        return num, symbol\n",
    "\n",
    "    try:\n",
    "        parsed_data = df[col_name].apply(parse_money_string)\n",
    "        \n",
    "        nums = [x[0] if isinstance(x, tuple) else pd.NA for x in parsed_data]\n",
    "        symbols = [x[1] if isinstance(x, tuple) else \"\" for x in parsed_data]\n",
    "        \n",
    "        # Overwrite the original column with just the pure math numbers\n",
    "        df[col_name] = nums\n",
    "        \n",
    "        # 6. Apply the Scale Decision to the numbers\n",
    "        scale_suffix = \"\"\n",
    "        if decision.scale_decision == \"Billions\":\n",
    "            df[col_name] = df[col_name] / 1_000_000_000\n",
    "            scale_suffix = \"in billions\"\n",
    "        elif decision.scale_decision == \"Millions\":\n",
    "            df[col_name] = df[col_name] / 1_000_000\n",
    "            scale_suffix = \"in millions\"\n",
    "        elif decision.scale_decision == \"Thousands\":\n",
    "            df[col_name] = df[col_name] / 1_000\n",
    "            scale_suffix = \"in thousands\"\n",
    "\n",
    "        # 7. Final Formatting: Split column vs Single Currency Header\n",
    "        if decision.is_mixed_currency:\n",
    "            # Find exactly where the current column is located\n",
    "            col_idx = df.columns.get_loc(col_name)\n",
    "            \n",
    "            # Create the new column name\n",
    "            new_currency_col = f\"{col_name}_currency\"\n",
    "            \n",
    "            # Insert the symbols list as a new column directly to the right\n",
    "            df.insert(loc=col_idx + 1, column=new_currency_col, value=symbols)\n",
    "            \n",
    "            # Rename original number column if scaling was applied\n",
    "            if scale_suffix:\n",
    "                new_col_name = f\"{col_name} ({scale_suffix})\"\n",
    "                df.rename(columns={col_name: new_col_name}, inplace=True)\n",
    "                print(f\"       [Tool Success] Mixed currencies split into '{new_currency_col}'. Numbers renamed to '{new_col_name}'.\")\n",
    "            else:\n",
    "                print(f\"       [Tool Success] Mixed currencies split into '{new_currency_col}'.\")\n",
    "                \n",
    "        else:\n",
    "            # Single currency: Keep as floats, put currency in the header, no new column needed\n",
    "            parts = []\n",
    "            if decision.detected_currency and decision.detected_currency != \"Unknown\":\n",
    "                parts.append(decision.detected_currency)\n",
    "            if scale_suffix:\n",
    "                parts.append(scale_suffix)\n",
    "                \n",
    "            if parts:\n",
    "                header_addition = \" \".join(parts)\n",
    "                new_col_name = f\"{col_name} ({header_addition})\"\n",
    "                df.rename(columns={col_name: new_col_name}, inplace=True)\n",
    "                print(f\"       [Tool Success] Floats extracted. Renamed to '{new_col_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"       [Tool Error] Failed: {e}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "def money_agent_workflow(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    print(f\"  -> [Money Agent] Taking control of column: '{col_name}'\")\n",
    "    sample_data = df[col_name].dropna().head(10).tolist()\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Look at this sample of financial data from the column '{col_name}'.\n",
    "    Data sample: {sample_data}\n",
    "    \n",
    "    Your task:\n",
    "    1. Identify the primary currency being used (e.g., $, USD, €, Yen, \"dollars\", \"euros\"). \n",
    "       - CRITICAL RULE: If a currency is specified even just once in the sample, and NO OTHER currencies are mentioned, assume that single currency applies to the entire column.\n",
    "    2. Set `is_mixed_currency` to True ONLY if you see multiple DIFFERENT currencies (e.g., \"dollars\" in one row and \"eur\" in another).\n",
    "    3. Determine the best scale (\"None\", \"Thousands\", \"Millions\", \"Billions\").\n",
    "       - Evaluate the TRUE underlying numerical value. \"100 million\" means 100,000,000. \n",
    "       - If the true values are predominantly in the millions, you MUST choose \"Millions\".\n",
    "    4. Identify the decimal separator used in the numbers (\".\" or \",\").\n",
    "       - WARNING: Commas that group thousands (like \"200,000,000\") are NOT decimal separators. If a comma groups thousands, the decimal separator is \".\".\n",
    "       - Only choose \",\" if the comma specifically separates fractional cents at the very end of the number (e.g., \"1.500,00\").\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise financial data standardization agent.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=MoneyFormatDecision\n",
    "    )\n",
    "    \n",
    "    decision = response.choices[0].message.parsed\n",
    "    print(f\"     [Money Agent Decision] Mixed: {decision.is_mixed_currency} | Currency: {decision.detected_currency} | Scale: {decision.scale_decision}\")\n",
    "    \n",
    "    df = execute_money_formatting(df, col_name, decision)\n",
    "    return df\n",
    "\n",
    "\n",
    "def execute_int_formatting(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    print(f\"       [Tool Executing] Cleaning and truncating '{col_name}' to integers...\")\n",
    "    \n",
    "    def parse_int(val):\n",
    "        if pd.isna(val):\n",
    "            return pd.NA\n",
    "            \n",
    "        val_str = str(val).lower().replace(',', '').strip()\n",
    "        \n",
    "        try:\n",
    "            num = float(val_str)\n",
    "            return int(num)\n",
    "        except ValueError:\n",
    "            return pd.NA\n",
    "\n",
    "    try:\n",
    "        df[col_name] = df[col_name].apply(parse_int)\n",
    "        df[col_name] = df[col_name].astype('Int64')\n",
    "        \n",
    "        print(f\"       [Tool Success] Column '{col_name}' safely truncated to integers.\")\n",
    "    except Exception as e:\n",
    "        print(f\"       [Tool Error] Failed to process integers: {e}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "def int_agent_workflow(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    print(f\"  -> [Int Agent] Taking control of column: '{col_name}' (Bypassing LLM for deterministic math)\")\n",
    "    \n",
    "    df = execute_int_formatting(df, col_name)\n",
    "    return df\n",
    "\n",
    "def execute_float_formatting(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    print(f\"       [Tool Executing] Standardizing floats for '{col_name}'...\")\n",
    "    \n",
    "    # 1. Clean the data and convert to pure floats\n",
    "    def extract_float(val):\n",
    "        if pd.isna(val):\n",
    "            return pd.NA\n",
    "        val_str = str(val).lower().replace(',', '').strip()\n",
    "        try:\n",
    "            return float(val_str)\n",
    "        except ValueError:\n",
    "            return pd.NA\n",
    "            \n",
    "    raw_floats = df[col_name].apply(extract_float)\n",
    "    \n",
    "    # 2. Determine the maximum number of decimal places in the column\n",
    "    max_decimals = 0\n",
    "    for val in raw_floats.dropna():\n",
    "        # Convert float to string (e.g., 0.876 -> \"0.876\") and split at the dot\n",
    "        parts = str(val).split('.')\n",
    "        if len(parts) == 2:\n",
    "            decimals = len(parts[1])\n",
    "            if max_decimals < decimals:\n",
    "                max_decimals = decimals\n",
    "                \n",
    "    # 3. Format every number to match the max_decimals length\n",
    "    def pad_float(val):\n",
    "        if pd.isna(val):\n",
    "            return pd.NA\n",
    "        # This dynamically creates a format rule like \"{:.3f}\"\n",
    "        return f\"{val:.{max_decimals}f}\"\n",
    "        \n",
    "    df[col_name] = raw_floats.apply(pad_float)\n",
    "    \n",
    "    print(f\"       [Tool Success] Floats standardized to {max_decimals} decimal places.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def float_agent_workflow(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    print(f\"  -> [Float Agent] Taking control of column: '{col_name}' (Bypassing LLM)\")\n",
    "    \n",
    "    df = execute_float_formatting(df, col_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "class NameFormatDecision(BaseModel):\n",
    "    reasoning: str\n",
    "    entity_type: Literal[\"Human Names\", \"Locations/Other\"]\n",
    "    dominant_format: Literal[\"First Last\", \"Last First\", \"N/A\"]\n",
    "\n",
    "\n",
    "def execute_name_formatting(df: pd.DataFrame, col_name: str, decision: NameFormatDecision) -> pd.DataFrame:\n",
    "    print(f\"       [Tool Executing] Cleaning names. Type: {decision.entity_type}, Format: {decision.dominant_format}...\")\n",
    "    \n",
    "    def parse_name(val):\n",
    "        if pd.isna(val):\n",
    "            return pd.NA\n",
    "            \n",
    "        # 1. Standardize capitalization (e.g., \"JOHN smith\" -> \"John Smith\")\n",
    "        clean_name = str(val).strip().title()\n",
    "        \n",
    "        # 2. If it's a Location/Other, we just return the title-cased string\n",
    "        if decision.entity_type == \"Locations/Other\":\n",
    "            return clean_name\n",
    "            \n",
    "        # 3. Handle Human Names\n",
    "        # If there's a comma (e.g., \"Smith, John\"), split it and force \"First Last\"\n",
    "        if \",\" in clean_name:\n",
    "            parts = [p.strip() for p in clean_name.split(\",\")]\n",
    "            if len(parts) == 2:\n",
    "                return f\"{parts[1]} {parts[0]}\"\n",
    "                \n",
    "        # If the LLM determined the column is mostly \"Last First\" without commas (e.g., \"Smith John\")\n",
    "        if decision.dominant_format == \"Last First\":\n",
    "            parts = clean_name.split()\n",
    "            if len(parts) == 2:\n",
    "                # Flip it to \"First Last\"\n",
    "                return f\"{parts[1]} {parts[0]}\"\n",
    "                \n",
    "        # Default fallback: return as-is (already title-cased)\n",
    "        return clean_name\n",
    "\n",
    "    try:\n",
    "        df[col_name] = df[col_name].apply(parse_name)\n",
    "        print(f\"       [Tool Success] Column '{col_name}' standardized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"       [Tool Error] Failed to process names: {e}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def name_agent_workflow(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    print(f\"  -> [Name Agent] Taking control of column: '{col_name}'\")\n",
    "    \n",
    "    # Grab 10 rows to give the LLM enough pattern context\n",
    "    sample_data = df[col_name].dropna().head(10).tolist()\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Look at this sample of proper nouns from the column '{col_name}'.\n",
    "    Data sample: {sample_data}\n",
    "    \n",
    "    Your task:\n",
    "    1. Determine if this column primarily contains \"Human Names\" or \"Locations/Other\" (like cities, states, companies).\n",
    "    2. If it is \"Human Names\", deduce the dominant structural format.\n",
    "       - Are they mostly \"First Last\" (e.g., John Smith)?\n",
    "       - Are they mostly \"Last First\" (e.g., Smith John)?\n",
    "       - NOTE: If you see ambiguous names (like \"Harper Taylor\"), look at the other names in the sample to deduce the pattern.\n",
    "    3. If it is \"Locations/Other\", select \"N/A\" for the format.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise text standardization agent.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=NameFormatDecision\n",
    "    )\n",
    "    \n",
    "    decision = response.choices[0].message.parsed\n",
    "    print(f\"     [Name Agent Decision] Type: {decision.entity_type} | Dominant Format: {decision.dominant_format}\")\n",
    "    \n",
    "    df = execute_name_formatting(df, col_name, decision)\n",
    "    return df\n",
    "\n",
    "class HeaderDecision(BaseModel):\n",
    "    reasoning: str\n",
    "    header_row_index: int\n",
    "    header_col_index: int\n",
    "\n",
    "def header_detection_agent(file_path: str):\n",
    "    print(f\"\\n[Header Agent] Scanning '{file_path}' for the true table coordinates...\")\n",
    "    \n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df_raw = pd.read_csv(file_path, header=None, nrows=15)\n",
    "        else:\n",
    "            df_raw = pd.read_excel(file_path, header=None, nrows=15)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file for header detection: {e}\")\n",
    "        return 0, 0\n",
    "\n",
    "    # Fill NaN values with empty strings so the LLM can easily see the \"blank\" cells\n",
    "    df_raw = df_raw.fillna(\"\")\n",
    "    raw_sample = df_raw.to_dict(orient=\"records\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Look at the first 15 rows of this raw data file.\n",
    "    Real-world files often have titles, export dates, or blank rows at the very top. They also frequently have blank columns on the left.\n",
    "    \n",
    "    Your task is to identify the 2D starting coordinate of the ACTUAL data table:\n",
    "    1. `header_row_index`: The 0-based index of the row containing the column headers (e.g., 'Txn ID', 'Date', 'Amount').\n",
    "    2. `header_col_index`: The 0-based index of the column where the actual data starts (ignoring empty/blank columns to the left).\n",
    "    \n",
    "    Raw data sample: {raw_sample}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a data parsing agent specialized in finding table structures.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=HeaderDecision\n",
    "    )\n",
    "    \n",
    "    decision = response.choices[0].message.parsed\n",
    "    print(f\"     [Header Agent Decision] Table starts at Row {decision.header_row_index}, Column {decision.header_col_index}.\")\n",
    "    print(f\"     [Reasoning] {decision.reasoning}\")\n",
    "    \n",
    "    return decision.header_row_index, decision.header_col_index\n",
    "\n",
    "\n",
    "\n",
    "class MissingDataDecision(BaseModel):\n",
    "    reasoning: str\n",
    "    custom_na_strings_to_wipe: list[str]  # e.g., [\"-\", \".\", \"?\"]\n",
    "    remove_completely_empty_rows: bool\n",
    "    remove_completely_empty_columns: bool\n",
    "\n",
    "\n",
    "def execute_missing_data_cleaning(df: pd.DataFrame, decision: MissingDataDecision) -> pd.DataFrame:\n",
    "    print(f\"       [Tool Executing] Cleaning custom NAs, empty rows, and empty columns...\")\n",
    "    try:\n",
    "        # 1. Wipe custom NA strings\n",
    "        if decision.custom_na_strings_to_wipe:\n",
    "            def wipe_custom_na(val):\n",
    "                if isinstance(val, str) and val.strip() in decision.custom_na_strings_to_wipe:\n",
    "                    return pd.NA\n",
    "                return val\n",
    "            \n",
    "            df = df.map(wipe_custom_na)\n",
    "            print(f\"       [Tool Success] Wiped strings: {decision.custom_na_strings_to_wipe}\")\n",
    "        \n",
    "        # 2. Remove completely empty rows\n",
    "        if decision.remove_completely_empty_rows:\n",
    "            initial_rows = len(df)\n",
    "            df = df.dropna(axis=0, how='all')\n",
    "            rows_removed = initial_rows - len(df)\n",
    "            if rows_removed > 0:\n",
    "                print(f\"       [Tool Success] Dropped {rows_removed} completely empty rows.\")\n",
    "                \n",
    "        # 3. Remove completely empty columns\n",
    "        if decision.remove_completely_empty_columns:\n",
    "            initial_cols = len(df.columns)\n",
    "            # axis=1 tells Pandas to drop columns instead of rows\n",
    "            df = df.dropna(axis=1, how='all')\n",
    "            cols_removed = initial_cols - len(df.columns)\n",
    "            if cols_removed > 0:\n",
    "                print(f\"       [Tool Success] Dropped {cols_removed} completely empty columns.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"       [Tool Error] Failed to clean missing data: {e}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "import string\n",
    "\n",
    "def na_agent_workflow(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(f\"\\n[NA Agent] Scanning dataframe for custom missing values, empty rows, and empty columns...\")\n",
    "    \n",
    "    # Pre-scan the data for 1-2 character strings that are pure punctuation\n",
    "    potential_nas = set()\n",
    "    for col in df.columns:\n",
    "        str_vals = df[col].dropna().astype(str)\n",
    "        for val in str_vals:\n",
    "            val = val.strip()\n",
    "            if 0 < len(val) <= 2 and all(c in string.punctuation for c in val):\n",
    "                potential_nas.add(val)\n",
    "                \n",
    "    sample_data = df.head(10).to_dict(orient=\"records\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze this dataset sample to identify how missing data is represented.\n",
    "    Pandas has already handled standard 'NaN' and 'N/A' automatically.\n",
    "    \n",
    "    However, we detected these specific punctuation-only strings in the dataset: {list(potential_nas)}\n",
    "    \n",
    "    Your task:\n",
    "    1. Evaluate if any of these strings (like \"-\", \".\") are being used as placeholders for missing data. If so, add them to `custom_na_strings_to_wipe`.\n",
    "    2. Decide if completely empty rows (rows where every single column is missing) should be removed. For standard tables, this is usually True.\n",
    "    3. Decide if completely empty columns (columns where every single row is missing) should be removed. For standard tables, this is usually True.\n",
    "    \n",
    "    Data Sample:\n",
    "    {sample_data}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a data cleaning agent focused on missing values.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=MissingDataDecision\n",
    "    )\n",
    "    \n",
    "    decision = response.choices[0].message.parsed\n",
    "    print(f\"     [NA Agent Decision] Wipe strings: {decision.custom_na_strings_to_wipe} | Drop rows: {decision.remove_completely_empty_rows} | Drop cols: {decision.remove_completely_empty_columns}\")\n",
    "    \n",
    "    df = execute_missing_data_cleaning(df, decision)\n",
    "    return df\n",
    "\n",
    "class FeatureDescription(BaseModel):\n",
    "    feature_name: str\n",
    "    inferred_data_type: str  # e.g., \"Continuous Numeric\", \"Categorical\", \"Datetime\"\n",
    "    description: str\n",
    "\n",
    "class DatasetDescription(BaseModel):\n",
    "    general_summary: str\n",
    "    features: list[FeatureDescription]\n",
    "\n",
    "def dataset_description_agent(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(f\"\\n  -> [Description Agent] Analyzing the final dataset to generate feature documentation...\")\n",
    "    \n",
    "    # Grab a sample of the cleaned data\n",
    "    sample_data = df.head(5).to_dict(orient=\"list\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following sample of a cleaned dataset.\n",
    "    Your task is to generate a comprehensive data dictionary.\n",
    "    \n",
    "    1. Provide a 1-2 sentence `general_summary` of what this dataset represents.\n",
    "    2. For every single column in the dataset, create a `FeatureDescription` detailing:\n",
    "       - The exact column name.\n",
    "       - The conceptual data type (e.g., Categorical, Datetime, Continuous Numeric, Text).\n",
    "       - A clear, concise description of what the data represents based on the column name and the values.\n",
    "    \n",
    "    Data Sample:\n",
    "    {sample_data}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert data analyst and documentation agent.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=DatasetDescription\n",
    "    )\n",
    "    \n",
    "    decision = response.choices[0].message.parsed\n",
    "    print(f\"     [Description Agent] Summary: {decision.general_summary}\")\n",
    "    \n",
    "    # Convert the LLM's Pydantic objects into a list of dictionaries for Pandas\n",
    "    description_records = []\n",
    "    for f in decision.features:\n",
    "        description_records.append({\n",
    "            \"Feature Name\": f.feature_name,\n",
    "            \"Conceptual Data Type\": f.inferred_data_type,\n",
    "            \"Description\": f.description\n",
    "        })\n",
    "        \n",
    "    # Create the DataFrame for the new Excel sheet\n",
    "    description_df = pd.DataFrame(description_records)\n",
    "    print(f\"     [Description Agent] Successfully generated dictionary for {len(description_records)} features.\")\n",
    "    \n",
    "    return description_df\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_file = \"case_A1_sales_light_dirty_input.xlsx\"\n",
    "    \n",
    "    print(\"--- STARTING AGENTIC PIPELINE ---\")\n",
    "    \n",
    "    # 1. SCOUT\n",
    "    header_row, header_col = header_detection_agent(test_file)\n",
    "    \n",
    "    # 2. LOAD\n",
    "    print(f\"\\n[Orchestrator] Loading file starting at row {header_row}...\")\n",
    "    if test_file.endswith('.csv'):\n",
    "        df = pd.read_csv(test_file, header=header_row)\n",
    "    else:\n",
    "        df = pd.read_excel(test_file, header=header_row)\n",
    "        \n",
    "    # 3. CROP\n",
    "    if header_col > 0:\n",
    "        print(f\"[Orchestrator] Cropping {header_col} empty columns from the left...\")\n",
    "        df = df.iloc[:, header_col:]\n",
    "        \n",
    "    # 4. SWEEP: The NA Agent purges custom nulls and empty rows\n",
    "    df = na_agent_workflow(df)\n",
    "    \n",
    "    # 5. READ: Pass the sanitized dataframe to the Reader Agent\n",
    "    classified_types = reader_agent(df)\n",
    "    \n",
    "    # 6. ROUTE: Pass to Orchestrator\n",
    "    orchestrator_router(df, classified_types, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f59d4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first of january 2016', 'january second 2016', 'yesterday']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"first of january 2016\", \"january second 2016\", \"yesterday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f95959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f5afd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING AGENTIC PIPELINE (MCP + SDK) for cleaned_test_pipeline_mcp.xlsx ---\n",
      "\n",
      "[Orchestrator Summary]:\n",
      "Here's a summary of the actions taken for each column in 'cleaned_test_pipeline_mcp.xlsx':\n",
      "\n",
      "1. **Event Date**: Identified as a time column. Contains non-standardized entries like \"first of january 2016\" and \"yesterday,\" which require conversion to a standard date format `%d/%m/%Y`. \n",
      "\n",
      "2. **Revenue**: Identified as a money column. Successfully formatted with mixed currencies using a scale of millions.\n",
      "\n",
      "3. **entities**: Identified as an int column. Successfully formatted to integers.\n",
      "\n",
      "4. **Customer**: Identified as a name column. Successfully formatted with the dominant \"First Last\" name structure.\n",
      "\n",
      "For the Event Date, ambiguous entries like \"yesterday\" can be converted to explicit dates based on today's date. Would you like to proceed with that conversion?\n",
      "\n",
      "[Orchestrator Summary]:\n",
      "Here's a summary of the actions taken for each column in 'cleaned_test_pipeline_mcp.xlsx':\n",
      "\n",
      "1. **Event Date**: Identified as a time column. Contains non-standardized entries like \"first of january 2016\" and \"yesterday,\" which require conversion to a standard date format `%d/%m/%Y`. \n",
      "\n",
      "2. **Revenue**: Identified as a money column. Successfully formatted with mixed currencies using a scale of millions.\n",
      "\n",
      "3. **entities**: Identified as an int column. Successfully formatted to integers.\n",
      "\n",
      "4. **Customer**: Identified as a name column. Successfully formatted with the dominant \"First Last\" name structure.\n",
      "\n",
      "For the Event Date, ambiguous entries like \"yesterday\" can be converted to explicit dates based on today's date. Would you like to proceed with that conversion?\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "import string\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "from agents import Agent, Runner, function_tool\n",
    "from agents.mcp import MCPServerStdio\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import re\n",
    "\n",
    "# 1. Define the MCP Server for Data Formatting Tools\n",
    "# We will write this to a separate file and run it as a subprocess\n",
    "mcp_server_code = r\"\"\"\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import re\n",
    "import string\n",
    "from typing import Literal\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"data-formatting-tools\")\n",
    "\n",
    "def _read_file(file_path: str, header: int = 0) -> pd.DataFrame:\n",
    "    '''Helper to read both CSV and Excel files.'''\n",
    "    if file_path.endswith('.csv'):\n",
    "        return pd.read_csv(file_path, header=header)\n",
    "    else:\n",
    "        return pd.read_excel(file_path, header=header)\n",
    "\n",
    "def _save_file(df: pd.DataFrame, file_path: str, index: bool = False):\n",
    "    '''Helper to save both CSV and Excel files.'''\n",
    "    if file_path.endswith('.csv'):\n",
    "        df.to_csv(file_path, index=index)\n",
    "    else:\n",
    "        df.to_excel(file_path, index=index)\n",
    "\n",
    "@mcp.tool()\n",
    "def execute_header_detection(file_path: str) -> str:\n",
    "    '''Detect the true header row and starting column of a data table in a file.\n",
    "    Returns the header_row_index and header_col_index as a string like \"row:2,col:1\".\n",
    "    The raw preview of the first 15 rows is also returned for context.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "    '''\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df_raw = pd.read_csv(file_path, header=None, nrows=15)\n",
    "        else:\n",
    "            df_raw = pd.read_excel(file_path, header=None, nrows=15)\n",
    "        df_raw = df_raw.fillna(\"\")\n",
    "        raw_sample = df_raw.to_dict(orient=\"records\")\n",
    "        return f\"RAW_PREVIEW: {raw_sample}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file for header detection: {e}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def apply_header_and_crop(file_path: str, header_row_index: int, header_col_index: int) -> str:\n",
    "    '''Re-read the file with the correct header row and crop empty columns from the left.\n",
    "    Overwrites the file with the properly loaded and cropped data.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "        header_row_index: The 0-based row index of the true header.\n",
    "        header_col_index: The 0-based column index where data starts.\n",
    "    '''\n",
    "    try:\n",
    "        df = _read_file(file_path, header=header_row_index)\n",
    "        if header_col_index > 0:\n",
    "            df = df.iloc[:, header_col_index:]\n",
    "        _save_file(df, file_path)\n",
    "        return f\"Successfully applied header at row {header_row_index}, cropped {header_col_index} columns. Shape: {df.shape}. Columns: {list(df.columns)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error applying header/crop: {e}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def execute_na_cleaning(\n",
    "    file_path: str,\n",
    "    custom_na_strings_to_wipe: list[str],\n",
    "    remove_completely_empty_rows: bool,\n",
    "    remove_completely_empty_columns: bool\n",
    ") -> str:\n",
    "    '''Clean missing data: wipe custom NA placeholder strings, remove empty rows/columns.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "        custom_na_strings_to_wipe: List of strings to treat as NA (e.g., [\"-\", \".\", \"?\"]).\n",
    "        remove_completely_empty_rows: Whether to drop rows where all values are missing.\n",
    "        remove_completely_empty_columns: Whether to drop columns where all values are missing.\n",
    "    '''\n",
    "    try:\n",
    "        df = _read_file(file_path)\n",
    "        messages = []\n",
    "        \n",
    "        if custom_na_strings_to_wipe:\n",
    "            def wipe_custom_na(val):\n",
    "                if isinstance(val, str) and val.strip() in custom_na_strings_to_wipe:\n",
    "                    return pd.NA\n",
    "                return val\n",
    "            df = df.map(wipe_custom_na)\n",
    "            messages.append(f\"Wiped custom NA strings: {custom_na_strings_to_wipe}\")\n",
    "        \n",
    "        if remove_completely_empty_rows:\n",
    "            initial_rows = len(df)\n",
    "            df = df.dropna(axis=0, how='all')\n",
    "            rows_removed = initial_rows - len(df)\n",
    "            if rows_removed > 0:\n",
    "                messages.append(f\"Dropped {rows_removed} completely empty rows.\")\n",
    "                \n",
    "        if remove_completely_empty_columns:\n",
    "            initial_cols = len(df.columns)\n",
    "            df = df.dropna(axis=1, how='all')\n",
    "            cols_removed = initial_cols - len(df.columns)\n",
    "            if cols_removed > 0:\n",
    "                messages.append(f\"Dropped {cols_removed} completely empty columns.\")\n",
    "        \n",
    "        _save_file(df, file_path)\n",
    "        return f\"NA cleaning complete. {'; '.join(messages)}. Shape: {df.shape}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error cleaning NAs: {e}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def detect_potential_na_strings(file_path: str) -> str:\n",
    "    '''Pre-scan the dataset for short punctuation-only strings that might be NA placeholders.\n",
    "    Also returns a sample of the first 10 rows for context.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "    '''\n",
    "    try:\n",
    "        df = _read_file(file_path)\n",
    "        potential_nas = set()\n",
    "        for col in df.columns:\n",
    "            str_vals = df[col].dropna().astype(str)\n",
    "            for val in str_vals:\n",
    "                val = val.strip()\n",
    "                if 0 < len(val) <= 2 and all(c in string.punctuation for c in val):\n",
    "                    potential_nas.add(val)\n",
    "        \n",
    "        sample_data = df.head(10).to_dict(orient=\"records\")\n",
    "        return f\"POTENTIAL_NAS: {list(potential_nas)}\\nSAMPLE: {sample_data}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error detecting NAs: {e}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def execute_time_formatting(\n",
    "    file_path: str, \n",
    "    col_name: str, \n",
    "    target_format: Literal[\"%H:%M\", \"%H:%M:%S\", \"%S\", \"%d/%m/%Y\", \"%d/%m/%Y %H:%M\", \"%d/%m/%Y %H:%M:%S\", \"%m/%Y\", \"%Y\"]\n",
    ") -> str:\n",
    "    '''Format a time/date column in a file to a specific target format.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "        col_name: Name of the column to format.\n",
    "        target_format: The target strftime format.\n",
    "    '''\n",
    "    try:\n",
    "        df = _read_file(file_path)\n",
    "        \n",
    "        def parse_natural_language(date_str):\n",
    "            if pd.isna(date_str):\n",
    "                return pd.NaT\n",
    "                \n",
    "            clean_str = str(date_str).lower()\n",
    "            replacements = {\n",
    "                \"first\": \"1st\", \"second\": \"2nd\", \"third\": \"3rd\", \n",
    "                \"fourth\": \"4th\", \"fifth\": \"5th\", \"sixth\": \"6th\", \n",
    "                \"seventh\": \"7th\", \"eighth\": \"8th\", \"ninth\": \"9th\", \n",
    "                \"tenth\": \"10th\", \"eleventh\": \"11th\", \"twelfth\": \"12th\", \n",
    "                \"thirteenth\": \"13th\", \"fourteenth\": \"14th\", \"fifteenth\": \"15th\", \n",
    "                \"sixteenth\": \"16th\", \"seventeenth\": \"17th\", \"eighteenth\": \"18th\", \n",
    "                \"nineteenth\": \"19th\", \"twentieth\": \"20th\",\n",
    "                \"twenty-first\": \"21st\", \"twenty first\": \"21st\",\n",
    "                \"twenty-second\": \"22nd\", \"twenty second\": \"22nd\",\n",
    "                \"twenty-third\": \"23rd\", \"twenty third\": \"23rd\",\n",
    "                \"twenty-fourth\": \"24th\", \"twenty fourth\": \"24th\",\n",
    "                \"twenty-fifth\": \"25th\", \"twenty fifth\": \"25th\",\n",
    "                \"twenty-sixth\": \"26th\", \"twenty sixth\": \"26th\",\n",
    "                \"twenty-seventh\": \"27th\", \"twenty seventh\": \"27th\",\n",
    "                \"twenty-eighth\": \"28th\", \"twenty eighth\": \"28th\",\n",
    "                \"twenty-ninth\": \"29th\", \"twenty ninth\": \"29th\",\n",
    "                \"thirtieth\": \"30th\", \n",
    "                \"thirty-first\": \"31st\", \"thirty first\": \"31st\",\n",
    "                \"last\": \"last\"\n",
    "            }\n",
    "            for word, num in replacements.items():\n",
    "                clean_str = clean_str.replace(word, num)\n",
    "                \n",
    "            parsed = dateparser.parse(clean_str)\n",
    "            return parsed if parsed else pd.NaT\n",
    "\n",
    "        df[col_name] = df[col_name].apply(parse_natural_language)\n",
    "        df[col_name] = df[col_name].dt.strftime(target_format)\n",
    "        _save_file(df, file_path)\n",
    "        return f\"Successfully formatted column '{col_name}' to '{target_format}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error formatting time: {e}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def execute_money_formatting(\n",
    "    file_path: str, \n",
    "    col_name: str, \n",
    "    is_mixed_currency: bool, \n",
    "    detected_currency: str, \n",
    "    scale_decision: Literal[\"None\", \"Thousands\", \"Millions\", \"Billions\"], \n",
    "    decimal_separator: Literal[\".\", \",\"]\n",
    ") -> str:\n",
    "    '''Format a money/financial column in a file. For mixed currencies, a separate \n",
    "    currency column is inserted to the right. For single currencies, the currency \n",
    "    code is added to the column header.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "        col_name: Name of the column to format.\n",
    "        is_mixed_currency: True if multiple currencies are present.\n",
    "        detected_currency: The primary currency detected (e.g., 'USD', 'EUR').\n",
    "        scale_decision: The scale to apply.\n",
    "        decimal_separator: The decimal separator used in the raw data.\n",
    "    '''\n",
    "    try:\n",
    "        df = _read_file(file_path)\n",
    "        \n",
    "        def parse_money_string(val):\n",
    "            if pd.isna(val):\n",
    "                return pd.NA, \"\"\n",
    "                \n",
    "            val_str = str(val).lower().strip()\n",
    "            original_str = str(val).strip() \n",
    "            \n",
    "            symbol_match = re.search(r'([\\$€£¥]|(?:usd|eur|gbp|jpy|dollars?|euros?|pounds?|yen))', original_str, re.IGNORECASE)\n",
    "            raw_symbol = symbol_match.group(1).lower() if symbol_match else \"\"\n",
    "            \n",
    "            currency_map = {\n",
    "                \"dollar\": \"USD\", \"dollars\": \"USD\", \"$\": \"USD\", \"usd\": \"USD\",\n",
    "                \"euro\": \"EUR\", \"euros\": \"EUR\", \"eur\": \"EUR\", \"€\": \"EUR\",\n",
    "                \"pound\": \"GBP\", \"pounds\": \"GBP\", \"gbp\": \"GBP\", \"£\": \"GBP\",\n",
    "                \"yen\": \"JPY\", \"jpy\": \"JPY\", \"¥\": \"JPY\"\n",
    "            }\n",
    "            symbol = currency_map.get(raw_symbol, raw_symbol.upper())\n",
    "            \n",
    "            if decimal_separator == \",\":\n",
    "                val_str = val_str.replace('.', '').replace(',', '.')\n",
    "            else:\n",
    "                val_str = val_str.replace(',', '')\n",
    "                \n",
    "            if val_str.count('.') > 1:\n",
    "                parts = val_str.rsplit('.', 1)\n",
    "                val_str = parts[0].replace('.', '') + '.' + parts[1]\n",
    "                \n",
    "            match = re.search(r'[\\d\\.]+', val_str)\n",
    "            if not match:\n",
    "                return pd.NA, symbol\n",
    "            try:\n",
    "                num = float(match.group())\n",
    "            except ValueError:\n",
    "                return pd.NA, symbol\n",
    "                \n",
    "            isolated_words = re.sub(r'[\\d\\.\\,€\\$£¥]', ' ', val_str).split()\n",
    "            \n",
    "            if any(w in isolated_words for w in ['billion', 'billions', 'bill', 'bil', 'b']):\n",
    "                num *= 1_000_000_000\n",
    "            elif any(w in isolated_words for w in ['million', 'millions', 'mill', 'mil', 'm']):\n",
    "                num *= 1_000_000\n",
    "            elif any(w in isolated_words for w in ['thousand', 'thousands', 'k']):\n",
    "                num *= 1_000\n",
    "            elif any(w in isolated_words for w in ['cent', 'cents']):\n",
    "                num /= 100\n",
    "                \n",
    "            return num, symbol\n",
    "\n",
    "        parsed_data = df[col_name].apply(parse_money_string)\n",
    "        nums = [x[0] if isinstance(x, tuple) else pd.NA for x in parsed_data]\n",
    "        symbols = [x[1] if isinstance(x, tuple) else \"\" for x in parsed_data]\n",
    "        \n",
    "        df[col_name] = nums\n",
    "        \n",
    "        scale_suffix = \"\"\n",
    "        if scale_decision == \"Billions\":\n",
    "            df[col_name] = df[col_name] / 1_000_000_000\n",
    "            scale_suffix = \"in billions\"\n",
    "        elif scale_decision == \"Millions\":\n",
    "            df[col_name] = df[col_name] / 1_000_000\n",
    "            scale_suffix = \"in millions\"\n",
    "        elif scale_decision == \"Thousands\":\n",
    "            df[col_name] = df[col_name] / 1_000\n",
    "            scale_suffix = \"in thousands\"\n",
    "\n",
    "        if is_mixed_currency:\n",
    "            # Insert a separate currency column to the right (matching Cell 1 logic)\n",
    "            col_idx = df.columns.get_loc(col_name)\n",
    "            new_currency_col = f\"{col_name}_currency\"\n",
    "            df.insert(loc=col_idx + 1, column=new_currency_col, value=symbols)\n",
    "            \n",
    "            if scale_suffix:\n",
    "                new_col_name = f\"{col_name} ({scale_suffix})\"\n",
    "                df.rename(columns={col_name: new_col_name}, inplace=True)\n",
    "        else:\n",
    "            parts = []\n",
    "            if detected_currency and detected_currency != \"Unknown\":\n",
    "                parts.append(detected_currency)\n",
    "            if scale_suffix:\n",
    "                parts.append(scale_suffix)\n",
    "                \n",
    "            if parts:\n",
    "                header_addition = \" \".join(parts)\n",
    "                new_col_name = f\"{col_name} ({header_addition})\"\n",
    "                df.rename(columns={col_name: new_col_name}, inplace=True)\n",
    "\n",
    "        _save_file(df, file_path)\n",
    "        return f\"Successfully formatted money column '{col_name}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error formatting money: {e}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def execute_int_formatting(file_path: str, col_name: str) -> str:\n",
    "    '''Clean and truncate a column to integers.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "        col_name: Name of the column to format.\n",
    "    '''\n",
    "    try:\n",
    "        df = _read_file(file_path)\n",
    "        def parse_int(val):\n",
    "            if pd.isna(val):\n",
    "                return pd.NA\n",
    "            val_str = str(val).lower().replace(',', '').strip()\n",
    "            try:\n",
    "                num = float(val_str)\n",
    "                return int(num)\n",
    "            except ValueError:\n",
    "                return pd.NA\n",
    "\n",
    "        df[col_name] = df[col_name].apply(parse_int)\n",
    "        df[col_name] = df[col_name].astype('Int64')\n",
    "        _save_file(df, file_path)\n",
    "        return f\"Successfully formatted integer column '{col_name}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error formatting integers: {e}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def execute_float_formatting(file_path: str, col_name: str) -> str:\n",
    "    '''Standardize floats for a column.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "        col_name: Name of the column to format.\n",
    "    '''\n",
    "    try:\n",
    "        df = _read_file(file_path)\n",
    "        def extract_float(val):\n",
    "            if pd.isna(val):\n",
    "                return pd.NA\n",
    "            val_str = str(val).lower().replace(',', '').strip()\n",
    "            try:\n",
    "                return float(val_str)\n",
    "            except ValueError:\n",
    "                return pd.NA\n",
    "                \n",
    "        raw_floats = df[col_name].apply(extract_float)\n",
    "        \n",
    "        max_decimals = 0\n",
    "        for val in raw_floats.dropna():\n",
    "            parts = str(val).split('.')\n",
    "            if len(parts) == 2:\n",
    "                decimals = len(parts[1])\n",
    "                if max_decimals < decimals:\n",
    "                    max_decimals = decimals\n",
    "                    \n",
    "        def pad_float(val):\n",
    "            if pd.isna(val):\n",
    "                return pd.NA\n",
    "            return f\"{val:.{max_decimals}f}\"\n",
    "            \n",
    "        df[col_name] = raw_floats.apply(pad_float)\n",
    "        _save_file(df, file_path)\n",
    "        return f\"Successfully formatted float column '{col_name}' to {max_decimals} decimal places.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error formatting floats: {e}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def execute_name_formatting(\n",
    "    file_path: str, \n",
    "    col_name: str, \n",
    "    entity_type: Literal[\"Human Names\", \"Locations/Other\"], \n",
    "    dominant_format: Literal[\"First Last\", \"Last First\", \"N/A\"]\n",
    ") -> str:\n",
    "    '''Standardize proper nouns/names in a column.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "        col_name: Name of the column to format.\n",
    "        entity_type: 'Human Names' or 'Locations/Other'.\n",
    "        dominant_format: 'First Last', 'Last First', or 'N/A'.\n",
    "    '''\n",
    "    try:\n",
    "        df = _read_file(file_path)\n",
    "        def parse_name(val):\n",
    "            if pd.isna(val):\n",
    "                return pd.NA\n",
    "            clean_name = str(val).strip().title()\n",
    "            if entity_type == \"Locations/Other\":\n",
    "                return clean_name\n",
    "            if \",\" in clean_name:\n",
    "                parts = [p.strip() for p in clean_name.split(\",\")]\n",
    "                if len(parts) == 2:\n",
    "                    return f\"{parts[1]} {parts[0]}\"\n",
    "            if dominant_format == \"Last First\":\n",
    "                parts = clean_name.split()\n",
    "                if len(parts) == 2:\n",
    "                    return f\"{parts[1]} {parts[0]}\"\n",
    "            return clean_name\n",
    "\n",
    "        df[col_name] = df[col_name].apply(parse_name)\n",
    "        _save_file(df, file_path)\n",
    "        return f\"Successfully formatted name column '{col_name}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error formatting names: {e}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def execute_dataset_description(file_path: str, general_summary: str, features_json: str) -> str:\n",
    "    '''Save a dataset description as a second sheet in the Excel file.\n",
    "    The cleaned data goes to \"Cleaned_Data\" sheet and the description goes to \"dataset_description\" sheet.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel file.\n",
    "        general_summary: A 1-2 sentence summary of the dataset.\n",
    "        features_json: A JSON string representing a list of objects with keys \"Feature Name\", \"Conceptual Data Type\", \"Description\".\n",
    "    '''\n",
    "    import json\n",
    "    try:\n",
    "        df = _read_file(file_path)\n",
    "        features = json.loads(features_json)\n",
    "        desc_df = pd.DataFrame(features)\n",
    "        \n",
    "        if file_path.endswith('.csv'):\n",
    "            # For CSV, save description as a separate file\n",
    "            desc_path = file_path.replace('.csv', '_description.csv')\n",
    "            df.to_csv(file_path, index=False)\n",
    "            desc_df.to_csv(desc_path, index=False)\n",
    "            return f\"Saved cleaned data to '{file_path}' and description to '{desc_path}'.\"\n",
    "        else:\n",
    "            with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "                df.to_excel(writer, sheet_name=\"Cleaned_Data\", index=False)\n",
    "                desc_df.to_excel(writer, sheet_name=\"dataset_description\", index=False)\n",
    "            return f\"Saved multi-sheet file to '{file_path}' with Cleaned_Data and dataset_description sheets.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error saving description: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"data_formatting_server.py\", \"w\") as f:\n",
    "    f.write(mcp_server_code)\n",
    "\n",
    "# 2. Define local function tools for reading data\n",
    "@function_tool\n",
    "def read_column_sample(file_path: str, col_name: str, n: int = 10) -> str:\n",
    "    \"\"\"Read a sample of data from a specific column in an Excel or CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "        col_name: Name of the column to sample.\n",
    "        n: Number of rows to sample.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            df = pd.read_excel(file_path)\n",
    "        if col_name not in df.columns:\n",
    "            return f\"Column '{col_name}' not found. Available columns: {list(df.columns)}\"\n",
    "        sample = df[col_name].dropna().head(n).tolist()\n",
    "        return str(sample)\n",
    "    except Exception as e:\n",
    "        return f\"Error reading sample: {e}\"\n",
    "\n",
    "@function_tool\n",
    "def read_data_sample(file_path: str, n: int = 5) -> str:\n",
    "    \"\"\"Read a sample of the entire dataset from an Excel or CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "        n: Number of rows to sample.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            df = pd.read_excel(file_path)\n",
    "        sample = df.head(n).to_dict(orient=\"list\")\n",
    "        return str(sample)\n",
    "    except Exception as e:\n",
    "        return f\"Error reading sample: {e}\"\n",
    "\n",
    "@function_tool\n",
    "def get_columns(file_path: str) -> str:\n",
    "    \"\"\"Get the list of columns in an Excel or CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            df = pd.read_excel(file_path)\n",
    "        return str(list(df.columns))\n",
    "    except Exception as e:\n",
    "        return f\"Error reading columns: {e}\"\n",
    "\n",
    "# 3. Define the Agents\n",
    "\n",
    "# --- HEADER DETECTION AGENT (NEW — matches Cell 1's header_detection_agent) ---\n",
    "header_agent = Agent(\n",
    "    name=\"Header Detection Agent\",\n",
    "    instructions=(\n",
    "        \"You are a data parsing agent specialized in finding table structures.\\n\"\n",
    "        \"Real-world files often have titles, export dates, or blank rows at the very top. \"\n",
    "        \"They also frequently have blank columns on the left.\\n\\n\"\n",
    "        \"STEP 1: Use the `execute_header_detection` MCP tool to get a raw preview of the first 15 rows.\\n\"\n",
    "        \"STEP 2: Analyze the raw preview to identify the 2D starting coordinate of the ACTUAL data table:\\n\"\n",
    "        \"   - `header_row_index`: The 0-based index of the row containing the column headers (e.g., 'Txn ID', 'Date', 'Amount').\\n\"\n",
    "        \"   - `header_col_index`: The 0-based index of the column where the actual data starts (ignoring empty/blank columns to the left).\\n\"\n",
    "        \"STEP 3: Use the `apply_header_and_crop` MCP tool to re-read the file with the correct header and crop empty columns.\\n\\n\"\n",
    "        \"Return a summary of what you found and applied.\"\n",
    "    ),\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    ")\n",
    "\n",
    "# --- NA AGENT (NEW — matches Cell 1's na_agent_workflow) ---\n",
    "na_agent = Agent(\n",
    "    name=\"NA Agent\",\n",
    "    instructions=(\n",
    "        \"You are a data cleaning agent focused on missing values.\\n\"\n",
    "        \"Pandas has already handled standard 'NaN' and 'N/A' automatically.\\n\\n\"\n",
    "        \"STEP 1: Use the `detect_potential_na_strings` MCP tool to scan for punctuation-only strings \"\n",
    "        \"that might be NA placeholders, and to get a sample of the data.\\n\"\n",
    "        \"STEP 2: Evaluate if any of these strings (like '-', '.') are being used as placeholders for missing data.\\n\"\n",
    "        \"STEP 3: Use the `execute_na_cleaning` MCP tool with your decisions:\\n\"\n",
    "        \"   - `custom_na_strings_to_wipe`: list of strings to treat as NA\\n\"\n",
    "        \"   - `remove_completely_empty_rows`: True for standard tables\\n\"\n",
    "        \"   - `remove_completely_empty_columns`: True for standard tables\\n\\n\"\n",
    "        \"Return a summary of what was cleaned.\"\n",
    "    ),\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    ")\n",
    "\n",
    "# --- READER AGENT ---\n",
    "reader_agent = Agent(\n",
    "    name=\"Reader Agent\",\n",
    "    instructions=(\n",
    "        \"You are a precise data analysis agent. Your job is to classify columns in a dataset.\\n\"\n",
    "        \"Use the `read_data_sample` tool to get a sample of the data (first 5 rows).\\n\"\n",
    "        \"For each column, determine its data type based on the values.\\n\"\n",
    "        \"You are ONLY allowed to use these exact categories: 'time', 'money', 'int', 'string', 'float', 'name', 'unknown'.\\n\\n\"\n",
    "        \"CRITICAL DEFINITIONS:\\n\"\n",
    "        \"- 'time': Includes standard formats (2023-01-01, 14:30), timestamps, AND natural language dates (e.g., 'first of january 2016', 'Q1 2024', 'yesterday'). If the core meaning represents a date or time, it is 'time', NEVER 'string'.\\n\"\n",
    "        \"- 'money': Includes currency symbols ($100, €50), accounting formats, or financial abbreviations (100 USD) and natural language money expressions ('100 dollars', 'fifty euros'). If the core meaning represents a monetary value, it is 'money', NEVER 'string'.\\n\"\n",
    "        \"- 'int': Whole numbers without decimals.\\n\"\n",
    "        \"- 'float': Numbers containing decimals.\\n\"\n",
    "        \"- 'name': Proper nouns. This includes human names (John Smith, Smith, John), cities, states (Alabama), or company names.\\n\"\n",
    "        \"- 'string': General text, sentences, descriptions, or specific codes (e.g., ID-4552) that have no mathematical or temporal value.\\n\"\n",
    "        \"- 'unknown': Use this ONLY if the column is complete gibberish or you cannot confidently assign it to any other category.\\n\\n\"\n",
    "        \"IMPORTANT: You MUST return your result as a JSON object mapping each column name to its classified type.\\n\"\n",
    "        \"Example format: {\\\"Column A\\\": \\\"time\\\", \\\"Column B\\\": \\\"money\\\", \\\"Column C\\\": \\\"int\\\"}\\n\"\n",
    "        \"The order must match the columns from left to right. Return ONLY this JSON mapping, nothing else.\"\n",
    "    ),\n",
    "    tools=[read_data_sample, get_columns],\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    ")\n",
    "\n",
    "# --- TIME AGENT ---\n",
    "time_agent = Agent(\n",
    "    name=\"Time Agent\",\n",
    "    instructions=(\n",
    "        \"You are an expert data formatting agent specializing in time and dates.\\n\"\n",
    "        \"When given a file and a column name, first use `read_column_sample` to look at the data.\\n\"\n",
    "        \"Determine the appropriate standardized format for this data based on its granularity:\\n\"\n",
    "        \"- Hours and minutes: '%H:%M'\\n\"\n",
    "        \"- Hours, minutes, and seconds: '%H:%M:%S'\\n\"\n",
    "        \"- Just seconds: '%S'\\n\"\n",
    "        \"- Specific dates: '%d/%m/%Y'\\n\"\n",
    "        \"- Date and time: '%d/%m/%Y %H:%M'\\n\"\n",
    "        \"- Date and exact time: '%d/%m/%Y %H:%M:%S'\\n\"\n",
    "        \"- Month and year: '%m/%Y'\\n\"\n",
    "        \"- Year only: '%Y'\\n\"\n",
    "        \"Then use the `execute_time_formatting` MCP tool to apply the format.\\n\"\n",
    "        \"You MUST pass the file_path, col_name, and target_format to the tool.\"\n",
    "    ),\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    ")\n",
    "\n",
    "# --- MONEY AGENT ---\n",
    "money_agent = Agent(\n",
    "    name=\"Money Agent\",\n",
    "    instructions=(\n",
    "        \"You are a precise financial data standardization agent.\\n\"\n",
    "        \"When given a file and a column name, first use `read_column_sample` to look at the data (request at least 10 rows).\\n\"\n",
    "        \"Your task:\\n\"\n",
    "        \"1. Identify the primary currency being used (e.g., $, USD, €, Yen, 'dollars', 'euros').\\n\"\n",
    "        \"   - CRITICAL RULE: If a currency is specified even just once in the sample, and NO OTHER currencies are mentioned, assume that single currency applies to the entire column.\\n\"\n",
    "        \"2. Set `is_mixed_currency` to True ONLY if you see multiple DIFFERENT currencies (e.g., 'dollars' in one row and 'eur' in another).\\n\"\n",
    "        \"3. Determine the best scale ('None', 'Thousands', 'Millions', 'Billions').\\n\"\n",
    "        \"   - Evaluate the TRUE underlying numerical value. '100 million' means 100,000,000.\\n\"\n",
    "        \"   - If the true values are predominantly in the millions, you MUST choose 'Millions'.\\n\"\n",
    "        \"4. Identify the decimal separator used in the numbers ('.' or ',').\\n\"\n",
    "        \"   - WARNING: Commas that group thousands (like '200,000,000') are NOT decimal separators. If a comma groups thousands, the decimal separator is '.'.\\n\"\n",
    "        \"   - Only choose ',' if the comma specifically separates fractional cents at the very end of the number (e.g., '1.500,00').\\n\"\n",
    "        \"Then use the `execute_money_formatting` MCP tool to apply the formatting.\\n\"\n",
    "        \"You MUST pass all required parameters: file_path, col_name, is_mixed_currency, detected_currency, scale_decision, and decimal_separator.\"\n",
    "    ),\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    ")\n",
    "\n",
    "# --- NAME AGENT ---\n",
    "name_agent = Agent(\n",
    "    name=\"Name Agent\",\n",
    "    instructions=(\n",
    "        \"You are a precise text standardization agent specializing in proper nouns.\\n\"\n",
    "        \"When given a file and a column name, first use `read_column_sample` to look at the data (request at least 10 rows).\\n\"\n",
    "        \"Your task:\\n\"\n",
    "        \"1. Determine if this column primarily contains 'Human Names' or 'Locations/Other' (like cities, states, companies).\\n\"\n",
    "        \"2. If it is 'Human Names', deduce the dominant structural format.\\n\"\n",
    "        \"   - Are they mostly 'First Last' (e.g., John Smith)?\\n\"\n",
    "        \"   - Are they mostly 'Last First' (e.g., Smith John)?\\n\"\n",
    "        \"   - NOTE: If you see ambiguous names (like 'Harper Taylor'), look at the other names in the sample to deduce the pattern.\\n\"\n",
    "        \"3. If it is 'Locations/Other', select 'N/A' for the format.\\n\"\n",
    "        \"Then use the `execute_name_formatting` MCP tool to apply the formatting.\\n\"\n",
    "        \"You MUST pass all required parameters: file_path, col_name, entity_type, and dominant_format.\"\n",
    "    ),\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    ")\n",
    "\n",
    "# --- DESCRIPTION AGENT (NEW — matches Cell 1's dataset_description_agent) ---\n",
    "description_agent = Agent(\n",
    "    name=\"Description Agent\",\n",
    "    instructions=(\n",
    "        \"You are an expert data analyst and documentation agent.\\n\"\n",
    "        \"When given a file path, use `read_data_sample` to get a sample of the cleaned data.\\n\"\n",
    "        \"Then use `get_columns` to get all column names.\\n\\n\"\n",
    "        \"Your task is to generate a comprehensive data dictionary:\\n\"\n",
    "        \"1. Write a 1-2 sentence `general_summary` of what this dataset represents.\\n\"\n",
    "        \"2. For every single column in the dataset, create an entry with:\\n\"\n",
    "        \"   - 'Feature Name': The exact column name.\\n\"\n",
    "        \"   - 'Conceptual Data Type': e.g., Categorical, Datetime, Continuous Numeric, Text.\\n\"\n",
    "        \"   - 'Description': A clear, concise description of what the data represents.\\n\\n\"\n",
    "        \"Then use the `execute_dataset_description` MCP tool to save the description as a second sheet.\\n\"\n",
    "        \"You MUST pass:\\n\"\n",
    "        \"   - file_path: the path to the file\\n\"\n",
    "        \"   - general_summary: your 1-2 sentence summary\\n\"\n",
    "        \"   - features_json: a JSON string representing a list of objects with keys 'Feature Name', 'Conceptual Data Type', 'Description'\\n\\n\"\n",
    "        \"Example features_json: '[{\\\"Feature Name\\\": \\\"Age\\\", \\\"Conceptual Data Type\\\": \\\"Continuous Numeric\\\", \\\"Description\\\": \\\"The age of the person in years.\\\"}]'\"\n",
    "    ),\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    ")\n",
    "\n",
    "# 4. Define the Orchestrator\n",
    "async def run_agentic_pipeline(file_path: str):\n",
    "    server_path = str(Path(\"data_formatting_server.py\").resolve())\n",
    "    python_executable = sys.executable\n",
    "    \n",
    "    async with MCPServerStdio(\n",
    "        name=\"Data Formatting Tools\",\n",
    "        params={\n",
    "            \"command\": python_executable,\n",
    "            \"args\": [server_path],\n",
    "        },\n",
    "    ) as server:\n",
    "        \n",
    "        orchestrator = Agent(\n",
    "            name=\"Data Pipeline Orchestrator\",\n",
    "            instructions=(\n",
    "                \"You are the orchestrator of a data cleaning pipeline. Follow these steps EXACTLY in order:\\n\\n\"\n",
    "                \"STEP 1 — SCOUT: Use the `header_agent` to detect and apply the correct header row and crop empty columns.\\n\"\n",
    "                \"   Pass a message like: 'Detect the header and crop the file \\\"<file_path>\\\"'.\\n\\n\"\n",
    "                \"STEP 2 — SWEEP: Use the `na_agent` to scan for and clean custom NA placeholders, empty rows, and empty columns.\\n\"\n",
    "                \"   Pass a message like: 'Clean missing data in the file \\\"<file_path>\\\"'.\\n\\n\"\n",
    "                \"STEP 3 — READ: Use the `reader_agent` to classify ALL columns in the file. Pass the file_path to it.\\n\"\n",
    "                \"   The reader_agent will return a JSON mapping of column names to types.\\n\\n\"\n",
    "                \"STEP 4 — ROUTE: Process EACH column one by one in order, based on its classified type:\\n\"\n",
    "                \"   - 'time': Delegate to `time_agent`. Pass a message like: 'Format the time column \\\"<col_name>\\\" in file \\\"<file_path>\\\"'.\\n\"\n",
    "                \"   - 'money': Delegate to `money_agent`. Pass a message like: 'Format the money column \\\"<col_name>\\\" in file \\\"<file_path>\\\"'.\\n\"\n",
    "                \"   - 'name': Delegate to `name_agent`. Pass a message like: 'Format the name column \\\"<col_name>\\\" in file \\\"<file_path>\\\"'.\\n\"\n",
    "                \"   - 'int': Directly use the `execute_int_formatting` MCP tool with file_path and col_name. Do NOT use an agent.\\n\"\n",
    "                \"   - 'float': Directly use the `execute_float_formatting` MCP tool with file_path and col_name. Do NOT use an agent.\\n\"\n",
    "                \"   - 'string' or 'unknown': Bypass — do nothing, these require no formatting.\\n\\n\"\n",
    "                \"STEP 5 — DESCRIBE: Use the `description_agent` to generate a data dictionary and save it as a second sheet.\\n\"\n",
    "                \"   Pass a message like: 'Generate a dataset description for the file \\\"<file_path>\\\"'.\\n\\n\"\n",
    "                \"CRITICAL RULES:\\n\"\n",
    "                \"   - You MUST execute ALL 5 steps in the exact order above.\\n\"\n",
    "                \"   - For 'time', 'money', and 'name', you MUST delegate to the respective agents and NOT call the MCP formatting tools directly.\\n\"\n",
    "                \"   - For 'int' and 'float', you MUST call the MCP tools directly and NOT delegate to agents.\\n\"\n",
    "                \"   - Always pass BOTH file_path AND col_name when delegating or calling tools.\\n\"\n",
    "                \"   - Process columns in order from left to right.\\n\\n\"\n",
    "                \"STEP 6: After all steps are complete, summarize the actions taken.\"\n",
    "            ),\n",
    "            tools=[\n",
    "                get_columns,\n",
    "                read_data_sample,\n",
    "                header_agent.as_tool(\n",
    "                    tool_name=\"header_agent\",\n",
    "                    tool_description=\"Detect the true header row and starting column, then re-read and crop the file accordingly. Pass the file_path.\"\n",
    "                ),\n",
    "                na_agent.as_tool(\n",
    "                    tool_name=\"na_agent\",\n",
    "                    tool_description=\"Scan for custom NA placeholder strings and clean empty rows/columns. Pass the file_path.\"\n",
    "                ),\n",
    "                reader_agent.as_tool(\n",
    "                    tool_name=\"reader_agent\", \n",
    "                    tool_description=\"Classify the data types of ALL columns in the file. Pass the file_path. Returns a JSON mapping of column_name -> type.\"\n",
    "                ),\n",
    "                time_agent.as_tool(\n",
    "                    tool_name=\"time_agent\", \n",
    "                    tool_description=\"Format a time/date column. You MUST include both the file_path and col_name in your message to this agent.\"\n",
    "                ),\n",
    "                money_agent.as_tool(\n",
    "                    tool_name=\"money_agent\", \n",
    "                    tool_description=\"Format a money/financial column. You MUST include both the file_path and col_name in your message to this agent.\"\n",
    "                ),\n",
    "                name_agent.as_tool(\n",
    "                    tool_name=\"name_agent\", \n",
    "                    tool_description=\"Format a name/proper noun column. You MUST include both the file_path and col_name in your message to this agent.\"\n",
    "                ),\n",
    "                description_agent.as_tool(\n",
    "                    tool_name=\"description_agent\",\n",
    "                    tool_description=\"Generate a data dictionary for the cleaned dataset and save it as a second sheet. Pass the file_path.\"\n",
    "                ),\n",
    "            ],\n",
    "            mcp_servers=[server],\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "        )\n",
    "        \n",
    "        # Give sub-agents access to MCP server and local tools\n",
    "        header_agent.mcp_servers = [server]\n",
    "        header_agent.tools = []\n",
    "        \n",
    "        na_agent.mcp_servers = [server]\n",
    "        na_agent.tools = []\n",
    "        \n",
    "        time_agent.mcp_servers = [server]\n",
    "        time_agent.tools = [read_column_sample]\n",
    "        \n",
    "        money_agent.mcp_servers = [server]\n",
    "        money_agent.tools = [read_column_sample]\n",
    "        \n",
    "        name_agent.mcp_servers = [server]\n",
    "        name_agent.tools = [read_column_sample]\n",
    "        \n",
    "        description_agent.mcp_servers = [server]\n",
    "        description_agent.tools = [read_data_sample, get_columns]\n",
    "\n",
    "        print(f\"--- STARTING AGENTIC PIPELINE (MCP + SDK) for {file_path} ---\")\n",
    "        result = await Runner.run(\n",
    "            orchestrator,\n",
    "            f\"Please analyze and format the data in '{file_path}'. Process every column.\"\n",
    "        )\n",
    "        print(\"\\n[Orchestrator Summary]:\")\n",
    "        print(result.final_output)\n",
    "\n",
    "# 5. Run the pipeline\n",
    "test_file_mcp = \"cleaned_test_pipeline_mcp.xlsx\"\n",
    "pd.DataFrame({\n",
    "    \"Event Date\": [\"first of january 2016\", \"january second 2016\", \"yesterday\"],\n",
    "    \"Revenue\": [\"100 million dollars\", \"200000000\", \"300 mil eur\"],\n",
    "    \"entities\": [5, 10, 15.0],\n",
    "    \"Customer\": [\"Alice\", \"Bob\", \"Charlie SMITH\"]\n",
    "}).to_excel(test_file_mcp, index=False)\n",
    "\n",
    "# Run the async function\n",
    "await run_agentic_pipeline(test_file_mcp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackeurope (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
